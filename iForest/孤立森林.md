# 孤立森林


## 孤立树的特点
孤立森林作为孤立树的总体，将具有较短路径长度的点识别为异常点，不同的树扮演不同异常识别的专家。

已经存在的那些异常检测的方法大部分都期望有更多的数据，但是在孤立森林中，**小数据集往往能取得更好的效果**。样本数较多会降低孤立森林孤立异常点的能力，因为正常样本会干扰隔离的过程，降低隔离异常的能力。子采样就是在这种情况下被提出的。

swamping和masking是异常检测中比较关键的问题。swamping指的是错误地将正常样本预测为异常。当正常样本很靠近异常样本时，隔离异常时需要的拆分次数会增加，使得从正常样本中区分出异常样本更加困难。masking指的是存在大量异常点隐藏了他们的本来面目。当异常簇比较大，并且比较密集时，同样需要更多的拆分才能将他们隔离出来。上面的这两种情况使得孤立异常点变得更加困难。

造成上面两种情况的原因都与数据量太大有关。孤立树的独有特点使得孤立森林能够通过子采样建立局部模型，减小swamping和masking对模型效果的影响。其中的原因是：子采样（sub-sampling）可以控制每棵孤立树的数据量；每棵孤立树专门用来识别特定的子样本。

## 孤立树

定义：假设T是孤立树的一个节点，它要么是没有子节点的叶子节点，要么是只有两个子节点$(T_l,T_r)$的内部节点。每一步分割，都包含特征$q$和分割值$p$，将$q<p$的数据分到$T_l$，将$q > p$的数据分到$T_r$。

给定n个样本数据$X={x_1,⋯,x_n}$，特征的维度为$d$。为了构建一棵孤立树，需要**随机选择一个特征$q$及其分割值$p$**，递归地分割数据集$X$，直到满足以下任意一个条件：(1)树达到了限制的高度；(2)节点上只有一个样本；(3)节点上的样本所有特征都相同。

异常检测的任务是给出一个反应异常程度的排序，常用的排序方法是根据样本点的路径长度或异常得分来排序，**异常点就是排在最前面的那些点**。

## 算法流程
孤立森林算法有两个阶段：训练阶段和测试阶段。

### 训练阶段
![](https://github.com/VentureRice/AnomalyDetection/blob/main/iForest/train.png)

输入参数：训练数据 $data$, 树的数量 $t$, 子采样样本量 $\psi$。

树的高度限制：$\phi=ceiling(log_2\psi)$

停止条件：(1)树达到了限制的高度；(2)节点上只有一个样本；(3)节点上的样本所有特征都相同。

### 测试阶段

样本点x的路径长度$h(x)$为从iTree的根节点到叶子节点所经过的边的数量。

在评估阶段，每一个测试样本的异常分数由期望路径长度$E(h(x))$得到，$E(h(x))$是将样本通过孤立森林中的每一棵树得到的。

### Anomaly Score

给定一个包含n个样本的数据集，树的平均路径长度为:
$$c(n)=2H(n−1)−2(n−1)/n$$

其中$H(i)$为调和数，该值可以被估计为$ln(i)+0.5772156649$。$c(n)$为给定样本数$n$时，路径长度的平均值，用来标准化样本$x$的路径长度$h(x)$。

样本x的异常得分定义为

$$s(x,n)=2^{\frac{−E(h(x))}{c(n)}}$$
其中，$E(h(x))$为样本x在一批孤立树中的路径长度的期望。下图给出了$s$和$E(h(x))$的关系。

